# ğŸ“˜ **AWS Serverless Netflix ML Pipeline**

This project implements a **fully serverless, end-to-end machine learning pipeline** using **AWS Lambda, Step Functions, S3, DynamoDB**, and **CloudWatch**, powered by a Kaggle Netflix dataset.

It performs:

* **ETL & Feature Engineering**
* **Model Inference**
* **Orchestrated workflow (ETL â†’ Inference)**
* **Predictions storage**
* **Pipeline run logging**

All without any servers â€” purely serverless + free-tier friendly.

---

# ğŸ“‚ **Architecture Overview**

## ğŸ§  **High-Level Flow**

1. Raw CSV uploaded to **S3 â†’ triggers ETL Lambda**
2. Lambda cleans data + engineering features â†’ outputs processed CSV
3. **Step Functions** orchestrates:

   * RunETL
   * RunInference
   * Success
4. Inference Lambda:

   * loads ML model
   * runs predictions
   * saves output to S3
   * logs metadata to DynamoDB
5. CloudWatch logs all steps

---

# ğŸ–¼ **Architecture Diagram**

![Input Image](image.png)


# ğŸš€ **Features**

### âœ” Fully Serverless

No EC2, no Kubernetes â€” all Lambda, S3, Step Functions.

### âœ” Automated ETL

Data cleaned + engineered automatically upon upload.

### âœ” Automated ML Inference

Batch predictions processed serverlessly.

### âœ” Logged Pipeline Metadata

DynamoDB stores:

* run_id
* timestamps
* clean file path
* predictions location
* record counts

### âœ” Production-ready Workflow

Step Functions coordinates the pipeline end-to-end.

---

# ğŸ§° **AWS Services Used**

| AWS Service        | Purpose                               |
| ------------------ | ------------------------------------- |
| **S3**             | Raw + Processed + Predictions storage |
| **Lambda**         | ETL, Model Inference                  |
| **Step Functions** | Orchestration                         |
| **DynamoDB**       | Pipeline run logging                  |
| **IAM**            | Access control                        |
| **CloudWatch**     | Logs + Monitoring                     |

---

# âš™ï¸ **Pipeline Steps Explained**

### 1ï¸âƒ£ S3 Upload

User uploads:

```
incoming/netflix_titles.csv
```

### 2ï¸âƒ£ ETL Lambda Triggered

Cleans data + feature engineering â†’ saves:

```
clean/netflix_titles_clean.csv
```

### 3ï¸âƒ£ Step Functions Runs

* RunETL
* RunInference
* SuccessState

### 4ï¸âƒ£ ML Inference Lambda

* Loads model
* Predicts
* Saves:

```
predictions/netflix_titles_predictions.csv
```

### 5ï¸âƒ£ DynamoDB Log

Stores metadata of each run.

---

# ğŸ’» **Local Environment Setup**

Install dependencies:

```bash
pip install pandas scikit-learn boto3 joblib
```

Train model:

```bash
python model_training/train_model.py
```

# **Output**

**S3 Bucket**

![Input Image](s31.png)

![Input Image](s32.png)

**Lambda**

![Input Image](lambda.png)

![Input Image](code_result.png)

![Input Image](lambda1.png)

![Input Image](code_result1.png)

**DynamoDB**

![Input Image](dynamo.png)

**Step Functions - ELT Pipeline**

![Input Image](etl1.png)

![Input Image](etl_pipeline.png)

![Input Image](etl_graph.png)