# ğŸ—ï¸ **Netflix Azure Data Engineering Project â€” End-to-End ETL Pipeline using Databricks Auto Loader and Delta Live Tables**

### ğŸ¬ Project Overview

This project demonstrates a **real-time data engineering pipeline on Azure** using **Databricks**, **Delta Lake**, and **Azure Data Lake Storage (ADLS Gen2)**.

The pipeline ingests, processes, and transforms Netflix dataset files through **Bronzeâ€“Silverâ€“Gold architecture**, leveraging **Auto Loader** for incremental ingestion and **Delta Live Tables (DLT)** for quality enforcement and pipeline orchestration.

It showcases how modern data platforms handle **incremental data loading, schema evolution, transformation, and validation** efficiently and reliably.

---

### ğŸ§© **Architecture Layers**

| Layer            | Technology                         | Description                                                                       |
| ---------------- | ---------------------------------- | --------------------------------------------------------------------------------- |
| **Raw (Source)** | CSV files in ADLS Gen2             | Unprocessed Netflix datasets.                                                     |
| **Bronze**       | Databricks Auto Loader (Streaming) | Incrementally ingests raw CSV data into Delta format.                             |
| **Silver**       | Spark SQL & PySpark                | Cleans, transforms, and enriches data for analytics.                              |
| **Gold**         | Delta Live Tables (DLT)            | Applies business logic, data quality rules, and generates analytics-ready tables. |

---

### âš™ï¸ **Technologies Used**

* **Azure Databricks**
* **Azure Data Lake Storage Gen2 (ADLS)**
* **PySpark / Spark SQL**
* **Delta Lake & Delta Live Tables (DLT)**
* **Auto Loader (cloudFiles)**
* **Databricks Jobs & Widgets**
* **Python & Structured Streaming**

---

### ğŸ“ **Project Structure**

```
Netflix_Azure_Data_Engineering_Project/
â”‚
â”œâ”€â”€ 1_Autoloader.dbc                 # Incremental ingestion from Raw â†’ Bronze using Auto Loader
â”œâ”€â”€ 2_Silver.dbc                     # Silver layer transformations
â”œâ”€â”€ 3_lookupNotebook.dbc             # Lookup arrays for metadata
â”œâ”€â”€ 4_Silver.dbc                     # Feature engineering and aggregation
â”œâ”€â”€ 5_lookupNotebook.dbc             # Parameter passing for jobs
â”œâ”€â”€ 6_falsenotebook.dbc              # Task value handling between jobs
â”œâ”€â”€ 7_DLT_Notebook.dbc               # Gold layer pipeline using DLT
â”œâ”€â”€ RawData/                         # Source Netflix CSV files
â”‚    â”œâ”€â”€ netflix_titles.csv
â”‚    â”œâ”€â”€ netflix_cast.csv
â”‚    â”œâ”€â”€ netflix_countries.csv
â”‚    â”œâ”€â”€ netflix_directors.csv
â”‚    â”œâ”€â”€ netflix_category.csv
â””â”€â”€ README.md                        # Documentation
```

---

### ğŸš€ **Pipeline Workflow**

#### ğŸ¥‰ **1ï¸âƒ£ Bronze Layer (Incremental Ingestion)**

**Notebook:** `1_Autoloader.dbc`

* Uses **Databricks Auto Loader** to continuously ingest CSV files from ADLS Gen2 raw container.
* Schema tracking via **checkpoint** for new files.

```python
df = spark.readStream.format("cloudFiles")\
  .option("cloudFiles.format", "csv")\
  .option("cloudFiles.schemaLocation", checkpoint_location)\
  .load("abfss://raw@netflixprojectdlansh.dfs.core.windows.net")

df.writeStream.option("checkpointLocation", checkpoint_location)\
  .trigger(processingTime='10 seconds')\
  .start("abfss://bronze@netflixprojectdlansh.dfs.core.windows.net/netflix_titles")
```

âœ… *Result:* Raw Netflix data stored as Delta tables in **Bronze** layer.

---

#### ğŸ¥ˆ **2ï¸âƒ£ Silver Layer (Data Transformation)**

**Notebook:** `2_Silver.dbc` & `4_Silver.dbc`

* Reads Bronze data, cleans nulls, casts data types, and derives new columns (`Shorttitle`, `type_flag`, etc.).
* Uses **window functions** for duration ranking.
* Stores transformed data in **Silver container** as Delta tables.

---

#### ğŸ§© **3ï¸âƒ£ Lookup Tables**

**Notebook:** `3_lookupNotebook.dbc`

* Defines lookup arrays for `directors`, `cast`, `countries`, and `category` datasets.
* Uses Databricks job utilities to dynamically pass values to downstream tasks.

---

#### ğŸª™ **4ï¸âƒ£ Gold Layer (Delta Live Tables)**

**Notebook:** `7_DLT_Notebook.dbc`

* Implements **Delta Live Tables (DLT)** for real-time transformation and validation.
* Uses **expectations (data quality rules)** such as `show_id IS NOT NULL`.
* Creates live streaming tables like `gold_netflixdirectors`, `gold_netflixcountries`, `gold_netflixtitles`.

```python
@dlt.table
@dlt.expect_all_or_drop({"rule1": "show_id is NOT NULL"})
def gold_netflixtitles():
    df = spark.readStream.table("LIVE.gold_trns_netflixtitles")
    return df
```

âœ… *Result:* Gold Delta tables ready for analytics and reporting.

---

### ğŸ§  **Key Features**

* ğŸ” **Incremental streaming ingestion** using Auto Loader.
* ğŸ§¹ **Data cleaning and standardization** with PySpark transformations.
* ğŸ§± **Delta Lake ACID compliance** for reliable table operations.
* âœ… **DLT-based data quality validation** ensuring clean production datasets.
* ğŸ”§ **Dynamic parameter passing** across notebooks using Databricks widgets and job utilities.
* ğŸ“ˆ **Multi-layer data architecture (Bronze â†’ Silver â†’ Gold)** aligning with the Medallion Architecture pattern.

---

### ğŸ’¾ **Sample Data Sources**

* `netflix_titles.csv`
* `netflix_cast.csv`
* `netflix_directors.csv`
* `netflix_countries.csv`
* `netflix_category.csv`

---

### ğŸ§° **How to Run**

1. Upload `.dbc` notebooks to Databricks workspace.
2. Connect Databricks to **Azure Data Lake Storage Gen2** via **Service Principal or Key Vault**.
3. Update storage paths for:

   * Raw data â†’ `abfss://raw@...`
   * Bronze â†’ `abfss://bronze@...`
   * Silver â†’ `abfss://silver@...`
4. Execute notebooks in order:

   * `1_Autoloader.dbc`
   * `2_Silver.dbc`
   * `4_Silver.dbc`
   * `7_DLT_Notebook.dbc`

---

### ğŸ“Š **Visualization & Insights**

Example visualization using:

```python
df_vis = df.groupBy("type").agg(count("*").alias("total_count"))
display(df_vis)
```

Produces insights on total **Movies vs TV Shows** on Netflix.

---

### ğŸ§¾ **Learning Outcomes**

âœ… Design and implement **end-to-end ETL pipelines** using Databricks & ADLS.
âœ… Understand the **Medallion Architecture (Bronze, Silver, Gold)** pattern.
âœ… Use **Auto Loader for streaming ingestion** and **DLT for validation**.
âœ… Leverage **PySpark transformations and window functions** for analytics.
âœ… Apply **data quality rules and governance** in DLT pipelines.

---

### ğŸ‘©â€ğŸ’» **Author**

**Pooja**
\
*Data Engineer | Cloud & DevOps Enthusiast*
\
ğŸ“ Project: *Netflix Azure Data Engineering Pipeline using Auto Loader & Delta Live Tables*