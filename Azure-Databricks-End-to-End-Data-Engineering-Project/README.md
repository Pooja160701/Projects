# ğŸš€ **Azure Databricks End-to-End Data Engineering Project**

### **Bronze â†’ Silver â†’ Gold Pipeline with Auto Loader, DLT, SCD Type-1 & Fact/Dim Modelling**

---

## ğŸ“Œ **Project Overview**

This project demonstrates a complete **modern Lakehouse ETL architecture** using **Azure Databricks, ADLS Gen2, Delta Lake, Auto Loader, and Delta Live Tables (DLT)**.
It covers **dynamic ingestion**, **multi-layer Medallion architecture**, **surrogate key generation**, **SCD Type-1**, **Fact/Dim modelling**, **SQL & Python UDFs**, **window functions**, and **DLT expectations** for data quality.

This is a real-world project suitable for **production-scale retail/e-commerce ETL pipelines**.

---

## ğŸ§© **Architecture Summary**

```
                â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                â”‚   ADLS Gen2 â€“ Source     â”‚
                â”‚ (orders, customers, ...) â”‚
                â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                               â–¼
                 ğŸ” **Auto Loader (Streaming)**
                               â–¼
                â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                â”‚      Bronze Layer        â”‚
                â”‚ (Raw â†’ Parquet Streams)  â”‚
                â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                               â–¼
                 ğŸ”§ **Silver Transformations**
                 - Cleaning / Dedup
                 - Parsing fields
                 - Window functions
                 - OOP class transformations
                               â–¼
                â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                â”‚       Silver Layer       â”‚
                â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                               â–¼
                â­ **Gold Layer (DLT + SCD)**
                 - DimCustomers (SCD1)
                 - DimProducts (SCD2 via DLT)
                 - FactOrders (Upsert)
                               â–¼
                ğŸ“Š **Gold Presentation Tables**
```

---

## ğŸ—ï¸ **Technologies Used**

* **Azure Databricks**
* **Auto Loader (cloudFiles)**
* **Data Lake Storage Gen2 (ABFSS)**
* **Delta Lake (ACID tables)**
* **Delta Live Tables (DLT)**
* **PySpark (Structured Streaming + SQL API)**
* **Window Functions**
* **UDFs (SQL + Python-based UDF + Table-valued Functions)**
* **SCD Type-1 / Type-2**
* **Databricks Job Utilities (dbutils.jobs.taskValues)**

---

## ğŸ“ **Repository Structure**

```
Databricks_Retail_ETL_Project/
â”‚
â”œâ”€â”€ Bronze_Layer.dbc
â”œâ”€â”€ Silver_Customers.dbc
â”œâ”€â”€ Silver_Orders.dbc
â”œâ”€â”€ Silver_Products.dbc
â”œâ”€â”€ Silver_Regions.dbc
â”œâ”€â”€ Gold_Products_DLT.dbc
â”œâ”€â”€ Gold_Customers_SCD1.dbc
â”œâ”€â”€ Gold_Orders_Fact.dbc
â”œâ”€â”€ parameters.dbc
â””â”€â”€ README.md
```

---

# ğŸ¥‰ **Bronze Layer â€“ Auto Loader Dynamic Ingestion**

### âœ” Dynamic file ingestion using widget parameter

```python
dbutils.widgets.text("file_name","")
p_file_name = dbutils.widgets.get("file_name")
```

### âœ” Auto Loader streaming ingestion

```python
df = spark.readStream.format("cloudFiles")\
    .option("cloudFiles.format","parquet")\
    .option("cloudFiles.schemaLocation", f".../checkpoint_{p_file_name}")\
    .load(f".../source/{p_file_name}")
```

### âœ” Writing to Bronze Layer

```python
df.writeStream.format("parquet")\
    .option("path", f".../bronze/{p_file_name}")\
    .trigger(once=True)\
    .start()
```

---

# ğŸ¥ˆ **Silver Layer â€“ Transformations**

## Silver Customers

* Extract domain from email
* Create full name
* Remove rescued data
* Write Delta table

```python
df = df.withColumn("domains", split(col('email'),'@')[1])
df = df.withColumn("full_name", concat(col('first_name'),lit(' '),col('last_name')))
```

---

## Silver Orders

Includes:

* Clean data
* Timestamp conversion
* Year extraction
* Window functions
* **OOP-based window transformations**

### OOP Class Example

```python
class windows:
    def dense_rank(self,df):
        return df.withColumn("flag",dense_rank().over(Window.partitionBy("year").orderBy(desc("total_amount"))))
```

---

## Silver Products

* SQL UDF
* Python UDF
* Table-valued UDF

### SQL Function

```sql
CREATE OR REPLACE FUNCTION discount_func(p_price DOUBLE)
RETURN p_price * 0.90
```

### Python Function

```sql
CREATE OR REPLACE FUNCTION upper_func(p_brand STRING)
LANGUAGE PYTHON AS $$ return p_brand.upper() $$
```

---

# ğŸ¥‡ **Gold Layer â€“ Fact & Dimensions**

## â­ **DimCustomers â€“ SCD Type-1**

Steps:

1. Read Silver customers
2. Dedup using `dropDuplicates()`
3. Split old vs new records
4. Surrogate key generation
5. Merge using Delta Lake Upsert

### Surrogate Key Logic

```python
df_new = df_new.withColumn("DimCustomerKey", monotonically_increasing_id() + 1)
```

### Upsert into Delta Table

```python
dlt_obj.alias("trg").merge(df_final.alias("src"),
    "trg.DimCustomerKey = src.DimCustomerKey")\
    .whenMatchedUpdateAll()\
    .whenNotMatchedInsertAll()\
    .execute()
```

---

## â­ **DimProducts â€“ SCD Type-2 with DLT**

### DLT Table with Expectations

```python
@dlt.expect_all_or_drop({"rule1": "product_id IS NOT NULL"})
```

### SCD2 Logic

```python
dlt.apply_changes(
  target = "DimProducts",
  source = "DimProducts_view",
  keys = ["product_id"],
  sequence_by = "product_id",
  stored_as_scd_type = 2
)
```

---

## â­ **FactOrders â€“ Fact Table Upsert**

* Joins Silver orders with DimCustomers & DimProducts
* Creates fact record
* Uses Delta merge for Upsert

```python
dlt_obj.alias("trg").merge(df_fact_new.alias("src"),
   "trg.order_id = src.order_id AND trg.DimCustomerKey = src.DimCustomerKey")
```

---

# ğŸ§ª Parameters Notebook

Passes dataset list dynamically using:

```python
dbutils.jobs.taskValues.set("output_datasets", datasets)
```

---

## ğŸ¯ **Key Outcomes**

* Fully scalable **production-grade** ETL pipeline
* Implements **Lakehouse best practices**
* Automates ingestion â†’ cleaning â†’ modelling
* Uses **SCD Type-1, SCD Type-2, and Fact table upserts**
* Ensures **data quality with DLT expectations**